import { Llm } from '@trilogy-group/xoh-integration';
import { generateObject } from 'ai';
import { GoogleColab } from '../integrations/google-colab';
import { BmSubmission, BmSubmissionGradingResult, BmSubmissionSheetGrading } from './grade-bm-submission';
import { GradingRubric, SubmissionObjective } from './subbmission-details';
import { z } from 'zod';

const SubmissionGradingQuestionSchema = z.object({
  score: z.number().describe('Score for the grading rubric, 0 for 0-star, 1 for 1-star, etc'),
  reason: z.string().describe('The reasoning of such score, up to 600 symbols'),
});

const BmSubmissionGradingResultSchema = z.object({
  first: SubmissionGradingQuestionSchema.describe('Grading for the first rubric'),
  second: SubmissionGradingQuestionSchema.describe('Grading for the second rubric'),
  third: SubmissionGradingQuestionSchema.describe('Grading for the third rubric'),
});

export async function gradeJupyter(
  submission: BmSubmission,
  sheetGrading: BmSubmissionSheetGrading,
): Promise<BmSubmissionGradingResult> {
  let mdContent = null;
  try {
    mdContent = await GoogleColab.exportAsMarkdownOrJson(submission.jupiterLink);
  } catch (e) {
    throw new Error(`Cannot fetch Jupyter Notebook: ${e.message}`);
  }
  if (mdContent == null) {
    throw new Error('Unable to export Jupyter notebook as Markdown or JSON');
  }

  const llm = await Llm.getModel({
    model: 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',
    provider: 'bedrock',
  });

  const system = `
Your task is to assess the candidate's SUBMISSION.
The submission is provided as a Jupiter Notebook.
The submission objective is provided to you in the OBJECTIVE blocks.

<OBJECTIVE>
${SubmissionObjective}
</OBJECTIVE>

Candidate is also provided with the several boilerplate parts of the code, such as:
- Example of the OpenAI usage (creating a sample completion)
- Example of the input sheet reading
- Example of the output sheet writing

<PROVIDED_CODE>
# Code example of OpenAI communication

from openai import OpenAI

client = OpenAI(
    # In order to use provided API key, make sure that models you create point to this custom base URL.
    base_url='https://47v4us7kyypinfb5lcligtc3x40ygqbs.lambda-url.us-east-1.on.aws/v1/',
    # The temporary API key giving access to ChatGPT 4o model. Quotas apply: you have 500'000 input and 500'000 output tokens, use them wisely ;)
    api_key='<OPENAI API KEY: Use one provided by Crossover or your own>'
)

completion = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {"role": "user", "content": "Hello!"}
  ]
)

print(completion.choices[0].message)
</PROVIDED_CODE>

Your goal is to determine the score of the candidate submission based on the provided GRADING_RULES.

<GRADING_RULES>
${GradingRubric}
</GRADING_RULES>

The grading rules mention accuracy of the grading, it will be provided in the OUTPUT_ACCURACY block.

Additional instructions:
- LLM usage is a key part of the assessments. It can be direct call to the OpenAI APIs, library usage, or other libs like Langchain.
- It is important to ignore the provided parts (task description, boilerplate, etc.) and focus on the parts provided by the candidate.
- The email responses are supposed to be generated by LLM for the applicable grading rules.
- When choosing the rating (stars) make sure all the mentioned requirements are met.
- Ignore PROVIDED_CODE, as it is not part of the candidate submission.
- It is a common case when candidate do not submit anything or very minimal submission. Feel free to grade it as 0-star.

Think step by step.
  `;

  const user = `
<SUBMISSION>
${mdContent}
</SUBMISSION>

<OUTPUT_ACCURACY>
Email Classification Accuracy: ${sheetGrading.classification}
Order Request Accuracy: ${sheetGrading.orderStatus}
</OUTPUT_ACCURACY>
  `;

  const response = await generateObject({
    model: llm,
    system: system,
    prompt: user,
    schema: BmSubmissionGradingResultSchema,
  });

  return response.object as BmSubmissionGradingResult;
}
